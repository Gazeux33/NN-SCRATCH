# Neural Networks Library from Scratch

the goal in this project is to create a small and modular library to implements 
a classic neural networks. I was insipred by the architecure of pytorch and tensorflow
to create my classes and functions.

I'm doing this project because I'm still using libraries like pytorch and tensorflow and I wanted to implement a neural network myself from scratch.

## Features
### Layers
- ✅ Linear Layer

### Activation Functions
- ✅ ReLU
- ✅ Sigmoid
- ✅ Tanh
- ✅ Softmax

### Loss Functions
- ✅ MSE
- ✅ CrossEntropy
- ✅ BinaryCrossEntropy

### Optimizers
- ✅ SGD
- ✅ Adam


## Example of classification

Multi classification

<div>
	<img src="https://github.com/Gazeux33/NeuralNetwork/blob/main/assets/multi_class2.png" width="500">
</div>
<br>
<br>


Multi classification on the mnist dataset

<div>
	<img src="https://github.com/Gazeux33/NeuralNetwork/blob/main/assets/mnist.png" width="500">
</div>
<br>
<br>


Binary classification

<div>
	<img src="https://github.com/Gazeux33/NeuralNetwork/blob/main/assets/binary_class.png" width="500">
</div>







## Example of regression

Linear regression
<div>
	<img src="https://github.com/Gazeux33/NeuralNetwork/blob/main/assets/regression.png" width="500">
</div>








